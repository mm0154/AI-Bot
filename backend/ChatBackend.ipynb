{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn google-generativeai python-dotenv  pyngrok nest-asyncio\n",
        "!pip install websockets\n"
      ],
      "metadata": {
        "id": "t4CG8KUdYk6l",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from google.colab import userdata\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "import asyncio\n",
        "import websockets\n",
        "import nest_asyncio\n",
        "\n",
        "user_d = userdata.get('NGROK_AUTH')\n",
        "ngrok.set_auth_token(user_d)\n",
        "# Apply patch\n",
        "nest_asyncio.apply()\n",
        "\n",
        "connected = set()\n",
        "\n",
        "# async def echo(websocket, path):\n",
        "#     connected.add(websocket)\n",
        "#     try:\n",
        "#         async for message in websocket:\n",
        "#             print(f\"Received: {message}\")\n",
        "#             for conn in connected:\n",
        "#                 if conn != websocket:\n",
        "#                     await conn.send(f\"User says: {message}\")\n",
        "#     finally:\n",
        "#         connected.remove(websocket)\n",
        "\n",
        "# start_server = websockets.serve(echo, \"localhost\", 8765)\n",
        "\n",
        "# asyncio.get_event_loop().run_until_complete(start_server)\n",
        "# print(\"WebSocket server running on ws://localhost:8765\")\n",
        "# asyncio.get_event_loop().run_forever()\n",
        "\n",
        "# # Expose port 8765\n",
        "# public_url = ngrok.connect(8765, \"tcp\")\n",
        "# print(f\"WebSocket tunnel: {public_url}\")\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# CORS for frontend\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"http://localhost:5173\"],  # âœ… Match your React dev server\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Load Gemini API key\n",
        "genai.configure(api_key=userdata.get(\"GEMINI_APi_KEy\"))\n",
        "\n",
        "model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
        "\n",
        "class Message(BaseModel):\n",
        "    prompt: str\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat(message: Message):\n",
        "    response = model.generate_content(message.prompt)\n",
        "    return {\"response\": response.text}\n",
        "\n",
        "# Open a tunnel to the Uvicorn server\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Run the app\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "id": "ms--5bYbhB_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}